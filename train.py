import os
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm
from metrics.dataloader import RainDataset, get_transform

# Import modules
from models.archs.GFCS_X_mod1 import GFCSNetwork
from models.archs.losses import L1Loss
from models.CosineAnnealingRestartCyclicLR import CosineAnnealingRestartCyclicLR

# è¨­å®šè¶…åƒæ•¸
CONFIG = {
    "epochs": 100,  # è¨“ç·´è¼ªæ•¸
    "batch_size": 8,  # æ¯æ‰¹æ¬¡è™•ç†çš„åœ–ç‰‡æ•¸
    "lr": 3e-4,  # åˆå§‹å­¸ç¿’ç‡
    "eta_min": 1e-6,  # æœ€å°å­¸ç¿’ç‡
    "periods": [92000, 208000],  # é¤˜å¼¦è¡°æ¸›é€±æœŸ
    "restart_weights": [1, 1],  # é€±æœŸé‡å•Ÿæ™‚çš„å­¸ç¿’ç‡æ¯”ä¾‹
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "num_workers": 4,  # DataLoader é€²ç¨‹æ•¸
    "checkpoint_dir": "checkpoints",  # å­˜æ”¾æ¨¡å‹
    "log_interval": 10,  # å¹¾å€‹ batch è¨˜éŒ„ä¸€æ¬¡ Loss
    "use_amp": True,  # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
}

if __name__ == "__main__":
    # å‰µå»ºå­˜æ”¾æ¨¡å‹çš„è³‡æ–™å¤¾
    os.makedirs(CONFIG["checkpoint_dir"], exist_ok=True)

    # å‰µå»ºæ¨¡å‹
    model = GFCSNetwork(inp_channels=3, out_channels=3, dim=48)
    model.to(CONFIG["device"])
    model.half()  # âœ… ç¢ºä¿æ¨¡å‹åƒæ•¸ä½¿ç”¨ float16

    # å‰µå»ºæå¤±å‡½æ•¸
    criterion = L1Loss().to(CONFIG["device"])

    # å‰µå»ºå„ªåŒ–å™¨ & å­¸ç¿’ç‡èª¿æ•´å™¨
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG["lr"], 
                            betas=(0.9, 0.999), weight_decay=1e-4)
    scheduler = CosineAnnealingRestartCyclicLR(optimizer, periods=CONFIG["periods"], 
                                            restart_weights=CONFIG["restart_weights"], 
                                            eta_mins=[CONFIG["lr"], CONFIG["eta_min"]])

    # è¨­å®š DataLoader
    train_dataset = RainDataset(mode='train', dataset_name='Rain13K', 
                                transform=get_transform(train=True))
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, 
                                            shuffle=True, num_workers=4)

    # è¨­å®š AMPï¼ˆæ··åˆç²¾åº¦ï¼‰
    scaler = GradScaler(enabled=CONFIG["use_amp"])
    
    # è¨“ç·´è¿´åœˆ
    print(f"é–‹å§‹è¨“ç·´ GFCS_Xï¼Œä½¿ç”¨è¨­å‚™ï¼š{CONFIG['device']}")
    
    for epoch in range(CONFIG["epochs"]):
        model.train()
        running_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{CONFIG['epochs']}]", ncols=100)

        for batch_idx, (input_img, target_img) in enumerate(pbar):
            input_img, target_img = input_img.to(CONFIG["device"]).half(), target_img.to(CONFIG["device"]).half()  # âœ… ç¢ºä¿è¼¸å…¥æ˜¯ float16

            optimizer.zero_grad()

            # å‰å‘å‚³æ’­ï¼ˆAMP æ··åˆç²¾åº¦ï¼‰
            with torch.cuda.amp.autocast(enabled=CONFIG["use_amp"]):  # âœ… åœ¨ AMP æ¨¡å¼ä¸‹å‰å‘å‚³æ’­
                output = model(input_img)
                loss = criterion(output, target_img)

            # åå‘å‚³æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()  # æ›´æ–°å­¸ç¿’ç‡

            # è¨˜éŒ„ Loss
            running_loss += loss.item()
            if batch_idx % CONFIG["log_interval"] == 0:
                pbar.set_postfix(loss=loss.item())

        avg_loss = running_loss / len(train_loader)
        print(f"ğŸ”¹ Epoch [{epoch+1}/{CONFIG['epochs']}], Loss: {avg_loss:.6f}, LR: {scheduler.get_lr()[0]:.8f}")

        # ä¿å­˜æ¨¡å‹ï¼ˆæ¯ 10 å€‹ Epoch ä¿å­˜ä¸€æ¬¡ï¼‰
        if (epoch + 1) % 10 == 0:
            save_path = os.path.join(CONFIG["checkpoint_dir"], f"GFCS_X_epoch{epoch+1}.pth")
            torch.save(model.state_dict(), save_path)
            print(f"æ¨¡å‹å·²ä¿å­˜ï¼š{save_path}")

