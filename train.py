import os
import yaml
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
from tqdm import tqdm
from metrics.dataloader import RainDataset, get_transform

# Import modules
from models.archs.DPENet_v1 import DPENet
from models.archs.losses import SSIMLoss_v2, EdgeLoss_v2, L1Loss
from models.CosineAnnealingRestartCyclicLR import CosineAnnealingRestartCyclicLR

def load_config(file_path):
    with open(file_path, 'r') as file:
        config = yaml.safe_load(file)
    
    # Ensure specific types
    config["epochs"] = int(config["epochs"])
    config["batch_size"] = int(config["batch_size"])
    config["lr"] = float(config["lr"])
    config["eta_min"] = float(config["eta_min"])
    config["periods"] = [int(period) for period in config["periods"]]
    config["restart_weights"] = [float(weight) for weight in config["restart_weights"]]
    config["num_workers"] = int(config["num_workers"])
    config["log_interval"] = int(config["log_interval"])
    config["use_amp"] = bool(config["use_amp"])
    config["grad_clip"] = float(config["grad_clip"])  # æ–°å¢æ¢¯åº¦è£å‰ªé–¾å€¼ï¼Œé è¨­ç‚º 1.0
    
    return config

def denormalize(tensor, mean, std):
    """
    åæ­¸ä¸€åŒ– Tensor å°‡ Normalize(mean, std) è½‰å›åŸå§‹ç¯„åœ
    """
    mean = torch.tensor(mean).view(1, -1, 1, 1).to(tensor.device)  # èª¿æ•´ shape ä»¥åŒ¹é…è¼¸å…¥
    std = torch.tensor(std).view(1, -1, 1, 1).to(tensor.device)
    return tensor * std + mean  # åæ­¸ä¸€åŒ–

if __name__ == "__main__":
    # Load configuration from YAML file
    config = load_config('config.yml')
        
    # å‰µå»ºå­˜æ”¾æ¨¡å‹çš„è³‡æ–™å¤¾
    os.makedirs(config["checkpoint_dir"], exist_ok=True)

    # å‰µå»ºæ¨¡å‹
    model = DPENet()
    model.to(config["device"])

    # å‰µå»ºæå¤±å‡½æ•¸
    ssim_loss = SSIMLoss_v2().to(config["device"])
    edge_loss = EdgeLoss_v2().to(config["device"])
    l1_loss = L1Loss(loss_weight=1.0, reduction='mean').to(config["device"])

    # å‰µå»ºå„ªåŒ–å™¨ & å­¸ç¿’ç‡èª¿æ•´å™¨
    optimizer = optim.AdamW(model.parameters(), lr=config["lr"], 
                            betas=(0.9, 0.999), weight_decay=1e-4)
    scheduler = CosineAnnealingRestartCyclicLR(optimizer, periods=config["periods"], 
                                            restart_weights=config["restart_weights"], 
                                            eta_mins=[config["lr"], config["eta_min"]])

    # è¨­å®š DataLoader
    train_dataset = RainDataset(mode='train', dataset_name=config["dataset_name"], 
                                transform=get_transform(train=True))
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config["batch_size"], 
                                            shuffle=True, num_workers=4)

    # è¨­å®š AMPï¼ˆæ··åˆç²¾åº¦ï¼‰
    scaler = GradScaler('cuda', enabled=config["use_amp"])
    # è¨“ç·´è¿´åœˆ
    print(f"é–‹å§‹è¨“ç·´ DPENet_CFIMï¼Œä½¿ç”¨è¨­å‚™ï¼š{config['device']}")
    
    for epoch in range(config["epochs"]):
        model.train()
        running_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{config['epochs']}]")

        for batch_idx, (input_img, target_img) in enumerate(pbar):
            input_img, target_img = input_img.to(config["device"]), target_img.to(config["device"])

            optimizer.zero_grad()

            # å‰å‘å‚³æ’­ï¼ˆAMP æ··åˆç²¾åº¦ï¼‰
            with autocast(device_type='cuda', enabled=config["use_amp"]):
                output = model(input_img)
                ssim_val = ssim_loss(output, target_img)
                edge_val = edge_loss(output, target_img)
                if torch.isnan(ssim_val) or torch.isinf(ssim_val):
                    print("âš  SSIM Loss å‡ºéŒ¯", ssim_val)

                if torch.isnan(edge_val) or torch.isinf(edge_val):
                    print("âš  Edge Loss å‡ºéŒ¯", edge_val)

                loss = ssim_val + 0.05 * edge_val

            # åå‘å‚³æ’­
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–éœ‡ç›ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), config["grad_clip"])
            
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()  # æ›´æ–°å­¸ç¿’ç‡

            # è¨˜éŒ„ Loss
            running_loss += loss.item()
            if batch_idx % config["log_interval"] == 0:
                pbar.set_postfix(loss=loss.item())

        avg_loss = running_loss / len(train_loader)
        print(f"ğŸ”¹ Epoch [{epoch+1}/{config['epochs']}], Loss: {avg_loss:.6f}, LR: {scheduler.get_lr()[0]:.8f}")

        # ä¿å­˜æ¨¡å‹ï¼ˆæ¯ 10 å€‹ Epoch ä¿å­˜ä¸€æ¬¡ï¼‰
        if (epoch + 1) % 10 == 0:
            save_path = os.path.join(config["checkpoint_dir"], f"DPENet_CFIM_epoch{epoch+1}.pth")
            torch.save(model.state_dict(), save_path)
            print(f"æ¨¡å‹å·²ä¿å­˜ï¼š{save_path}")

