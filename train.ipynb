{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b8761-a5b5-49ab-9b93-7ebb58655e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import modules\n",
    "from GFCS_X_mod1 import GFCS_X, L1Loss\n",
    "from CosineAnnealingRestartCyclicLR import CosineAnnealingRestartCyclicLR\n",
    "# é€™è£¡æ‡‰è©²é‚„æœ‰ DataLoaderï¼Œä½†ä½ ä¹‹å¾Œæœƒè£œä¸Š\n",
    "# from dataset import CustomDataset \n",
    "\n",
    "# è¨­å®šè¶…åƒæ•¸\n",
    "CONFIG = {\n",
    "    \"epochs\": 100,  # è¨“ç·´è¼ªæ•¸\n",
    "    \"batch_size\": 8,  # æ¯æ‰¹æ¬¡è™•ç†çš„åœ–ç‰‡æ•¸\n",
    "    \"lr\": 3e-4,  # åˆå§‹å­¸ç¿’ç‡\n",
    "    \"eta_min\": 1e-6,  # æœ€å°å­¸ç¿’ç‡\n",
    "    \"periods\": [92000, 208000],  # é¤˜å¼¦è¡°æ¸›é€±æœŸ\n",
    "    \"restart_weights\": [1, 1],  # é€±æœŸé‡å•Ÿæ™‚çš„å­¸ç¿’ç‡æ¯”ä¾‹\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"num_workers\": 4,  # DataLoader é€²ç¨‹æ•¸\n",
    "    \"checkpoint_dir\": \"checkpoints\",  # å­˜æ”¾æ¨¡å‹\n",
    "    \"log_interval\": 10,  # å¹¾å€‹ batch è¨˜éŒ„ä¸€æ¬¡ Loss\n",
    "    \"use_amp\": True,  # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦\n",
    "}\n",
    "\n",
    "# å‰µå»ºå­˜æ”¾æ¨¡å‹çš„è³‡æ–™å¤¾\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹\n",
    "model = GFCS_X(inp_channels=3, out_channels=3, dim=48)\n",
    "model.to(CONFIG[\"device\"])\n",
    "model.half()  # âœ… ç¢ºä¿æ¨¡å‹åƒæ•¸ä½¿ç”¨ float16\n",
    "\n",
    "# å‰µå»ºæå¤±å‡½æ•¸\n",
    "criterion = L1Loss().to(CONFIG[\"device\"])\n",
    "\n",
    "# å‰µå»ºå„ªåŒ–å™¨ & å­¸ç¿’ç‡èª¿æ•´å™¨\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingRestartCyclicLR(\n",
    "    optimizer, periods=CONFIG[\"periods\"], restart_weights=CONFIG[\"restart_weights\"], eta_mins=[CONFIG[\"lr\"], CONFIG[\"eta_min\"]]\n",
    ")\n",
    "\n",
    "# è¨­å®š DataLoaderï¼ˆé€™éƒ¨åˆ†ç­‰ä½ æœ‰ Dataset å†è£œä¸Šï¼‰\n",
    "# train_dataset = CustomDataset(train_data_path)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"], pin_memory=True)\n",
    "\n",
    "# è¨­å®š AMPï¼ˆæ··åˆç²¾åº¦ï¼‰\n",
    "scaler = GradScaler(enabled=CONFIG[\"use_amp\"])\n",
    "\n",
    "# è¨“ç·´è¿´åœˆ\n",
    "def train():\n",
    "    print(f\"é–‹å§‹è¨“ç·´ GFCS_Xï¼Œä½¿ç”¨è¨­å‚™ï¼š{CONFIG['device']}\")\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{CONFIG['epochs']}]\", ncols=100)\n",
    "\n",
    "        for batch_idx, (input_img, target_img) in enumerate(pbar):\n",
    "            input_img, target_img = input_img.to(CONFIG[\"device\"]).half(), target_img.to(CONFIG[\"device\"]).half()  # âœ… ç¢ºä¿è¼¸å…¥æ˜¯ float16\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # å‰å‘å‚³æ’­ï¼ˆAMP æ··åˆç²¾åº¦ï¼‰\n",
    "            with torch.cuda.amp.autocast(enabled=CONFIG[\"use_amp\"]):  # âœ… åœ¨ AMP æ¨¡å¼ä¸‹å‰å‘å‚³æ’­\n",
    "                output = model(input_img)\n",
    "                loss = criterion(output, target_img)\n",
    "\n",
    "            # åå‘å‚³æ’­\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()  # æ›´æ–°å­¸ç¿’ç‡\n",
    "\n",
    "            # è¨˜éŒ„ Loss\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % CONFIG[\"log_interval\"] == 0:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"ğŸ”¹ Epoch [{epoch+1}/{CONFIG['epochs']}], Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "        # ä¿å­˜æ¨¡å‹ï¼ˆæ¯ 10 å€‹ Epoch ä¿å­˜ä¸€æ¬¡ï¼‰\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_path = os.path.join(CONFIG[\"checkpoint_dir\"], f\"GFCS_X_epoch{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"æ¨¡å‹å·²ä¿å­˜ï¼š{save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83afa7b3-56e6-4e2f-bca0-863c1a34ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train.ipynb to script\n",
      "[NbConvertApp] Writing 3183 bytes to train.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script train.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a8f9f-3253-47fb-9f1c-caff3cfe46f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310_env)",
   "language": "python",
   "name": "py310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
