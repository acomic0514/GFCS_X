{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import Norms as Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 影像轉token序列 (B, C, H, W) to (B, HW, C)\n",
    "def to_3d(x):\n",
    "    \"\"\"Reshape from (B, C, H, W) to (B, HW, C)\"\"\"\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "##########################################################################\n",
    "# token序列轉影像 (B, HW, C) to (B, C, H, W)\n",
    "def to_4d(x, h, w):\n",
    "    \"\"\"Reshape from (B, HW, C) to (B, C, H, W)\"\"\"\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "##########################################################################\n",
    "# MLP (flaoat16)\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(dim, 4*dim, bias=bias, dtype=torch.float16)\n",
    "        self.c_proj  = nn.Linear(4*dim, dim, bias=bias, dtype=torch.float16)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float16)\n",
    "        with torch.cuda.amp.autocast():  # ✅ AMP 自動管理精度\n",
    "            x = self.c_fc(x)\n",
    "            x = F.gelu(x)\n",
    "            x = self.c_proj(x)\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Token statistics transformer: linear-time attention via variational rate reduction\"\"\"\n",
    "##########################################################################\n",
    "# ToST（Token Statistics Transformer） 版本的自注意力，取代傳統的 QK 相似性計算\n",
    "class CausalSelfAttention_TSSA(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads = 8, block_size = 1024, dropout = 0.1, bias=False , dtype=torch.float16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # query, key, value projections\n",
    "        self.c_attn = nn.Linear(dim, dim, bias=bias, dtype=dtype)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(dim, dim, bias=bias, dtype=dtype)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = num_heads\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.block_size = block_size\n",
    "        self.temp = nn.Parameter(torch.ones((self.n_head, 1), dtype = dtype))\n",
    "        self.denom_bias = nn.Parameter(torch.zeros((self.n_head, block_size, 1), dtype = dtype))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, C) - token 序列\n",
    "        return: (B, N, C) - 經過 TSSA 處理的 token 序列\n",
    "        \"\"\"\n",
    "        x = x.to(torch.float16) # 確保計算在 float16 上執行\n",
    "        B, N, C = x.shape # batch size, sequence length, embedding dimensionality (dim)\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # ✅ AMP 自動管理精度\n",
    "            # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "            w = self.c_attn(x).view(B, N, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "            w_sq = w ** 2\n",
    "            denom = (torch.cumsum(w_sq,dim=-2)).clamp_min(torch.finfo(torch.float16).eps) # cumulative sum\n",
    "            w_normed = (w_sq / denom) + self.denom_bias[:,:N,:]\n",
    "        \n",
    "            # calculate attention weights\n",
    "            tmp = torch.sum(w_normed, dim=-1)* self.temp\n",
    "            Pi = F.softmax(tmp, dim=1) # B, nh, T\n",
    "        \n",
    "            # calculate attention\n",
    "            dots = torch.cumsum(w_sq * Pi.unsqueeze(-1), dim=-2) / (Pi.cumsum(dim=-1) + torch.finfo(torch.float16).eps).unsqueeze(-1)\n",
    "            attn = 1. / (1 + dots)\n",
    "            attn = self.attn_dropout(attn)\n",
    "        \n",
    "            # apply attention weights and combine heads\n",
    "            y = - torch.mul(w.mul(Pi.unsqueeze(-1)), attn)\n",
    "            y = y.transpose(1, 2).contiguous().view(B, N, C) # re-assemble all head outputs side by side\n",
    "            y = self.resid_dropout(self.c_proj(y))\n",
    "            \n",
    "        return y\n",
    "\n",
    "##########################################################################\n",
    "# ToST（Token Statistics Transformer）塊\n",
    "class ToSTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim = 1024, norm_type='WithBias'):\n",
    "        super().__init__()\n",
    "        self.ln_1 = Norms.Norm(dim, norm_type) # LayerNorm\n",
    "        self.attn = CausalSelfAttention_TSSA(dim) # TSSA\n",
    "        \n",
    "        self.ln_2 = Norms.Norm(dim, norm_type) # LayerNorm\n",
    "        self.mlp = MLP(dim)\n",
    "        eta = torch.finfo(torch.float16).eps\n",
    "        self.gamma1 = nn.Parameter(eta * torch.ones(dim), requires_grad=True)\n",
    "        self.gamma2 = nn.Parameter(eta * torch.ones(dim), requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) - 影像特徵圖\n",
    "        return: (B, C, H, W) - 經過 ToST 處理的影像特徵圖\n",
    "        \"\"\"\n",
    "        _, _, H, W = x.shape\n",
    "        \n",
    "        x = x + self.gamma1.view(1, -1, 1, 1) *to_4d(self.attn(self.ln_1(to_3d(x))), H, W)\n",
    "        x = x + self.gamma2.view(1, -1, 1, 1) *to_4d(self.mlp(self.ln_2(to_3d(x))), H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_1292\\130657001.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)  # ✅ 允許 AMP\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_1292\\130657001.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_1292\\3245774369.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # ✅ AMP 自動管理精度\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_1292\\1297796864.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # ✅ AMP 自動管理精度\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入形狀： torch.Size([2, 1024, 32, 32])\n",
      "to3d torch.Size([2, 1024, 1024])\n",
      "NORM torch.Size([2, 1024, 1024])\n",
      "CausalSelfAttention_TSSA torch.Size([2, 1024, 1024])\n",
      "to4d torch.Size([2, 1024, 32, 32])\n",
      "to3d torch.Size([2, 1024, 1024])\n",
      "MLP torch.Size([2, 1024, 1024])\n",
      "to4d torch.Size([2, 1024, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_1292\\130657001.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):  # ✅ AMP 運行\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ToSTBlock 測試通過，一切正常！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 設定測試參數\n",
    "B = 2       # batch size\n",
    "C = 1024    # 通道數 (與 dim 對應)\n",
    "H = 32      # 高度\n",
    "W = 32      # 寬度\n",
    "dtype = torch.float16  # 減少內存佔用\n",
    "\n",
    "# 建立 ToSTBlock\n",
    "tost_block = ToSTBlock(dim=C)  # ✅ 放到 GPU，確保 float16\n",
    "\n",
    "# 創建隨機輸入影像特徵 (B, C, H, W)\n",
    "x = torch.randn(B, C, H, W, dtype=dtype)  # ✅ 確保輸入數據是 float16\n",
    "\n",
    "# 設定 AMP（混合精度）\n",
    "scaler = GradScaler(enabled=True)  # ✅ 允許 AMP\n",
    "\n",
    "# 使用 AMP 進行前向運算\n",
    "with autocast(dtype=torch.float16):\n",
    "    y = tost_block(x)\n",
    "\n",
    "# 測試 1: 檢查輸出形狀是否正確\n",
    "assert y.shape == x.shape, f\"ToSTBlock 輸出形狀錯誤！預期 {x.shape}，但得到 {y.shape}\"\n",
    "\n",
    "# 測試 2: 檢查是否有 NaN 或 Inf\n",
    "assert not torch.isnan(y).any(), \"ToSTBlock 輸出包含 NaN！\"\n",
    "assert not torch.isinf(y).any(), \"ToSTBlock 輸出包含 Inf！\"\n",
    "\n",
    "# 測試 3: 反向傳播測試\n",
    "optimizer = torch.optim.Adam(tost_block.parameters(), lr=1e-3)  # ✅ 建立優化器\n",
    "optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "with autocast(dtype=torch.float16):  # ✅ AMP 運行\n",
    "    loss = y.mean()  # 假設損失函數是均值\n",
    "scaler.scale(loss).backward()  # ✅ 使用 AMP 反向傳播\n",
    "scaler.step(optimizer)  # ✅ AMP 更新權重\n",
    "scaler.update()  # ✅ AMP 調整 scale\n",
    "\n",
    "print(\"✅ ToSTBlock 測試通過，一切正常！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 100\n",
    "dtype = torch.float16\n",
    "position_bias = torch.randn((1, num_heads, 1, 1), dtype = dtype) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 1])\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(position_bias.shape)  # torch.Size([1, 100, 1, 1])\n",
    "print(position_bias.dtype)  # torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import Norms as Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# dilated dense residual block (DDRB)\n",
    "class DDRB(nn.Module):\n",
    "    \"\"\"\n",
    "    Dilated Dense Residual Block \n",
    "    Usage:\n",
    "        self.ddrb = DDRB(in_channels=32, mid_channels=32, kernel=3, stride=1, d=[1, 2, 5], bias=False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=32,\n",
    "                 mid_channels=32,\n",
    "                 kernel=3,\n",
    "                 stride=1,\n",
    "                 d=[1, 2, 5],\n",
    "                 bias=False):\n",
    "        super(DDRB, self).__init__()\n",
    "        self.convD1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[0], dilation=d[0], bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, mid_channels, kernel, stride, padding=d[0], dilation=d[0], bias=bias)\n",
    "            ) # dilation=1\n",
    "        self.convD2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[1], dilation=d[1], bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, mid_channels, kernel, stride, padding=d[1], dilation=d[1], bias=bias)\n",
    "            ) # dilation=2\n",
    "        self.convD3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[2], dilation=d[2], bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, mid_channels, kernel, stride, padding=d[2], dilation=d[2], bias=bias)\n",
    "            ) # dilation=5\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input feature map\n",
    "        Returns:\n",
    "            enhanced feature map\n",
    "        Usage:\n",
    "            enhanced_feature = DDRB(input_feature)\n",
    "        \"\"\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x1 = self.convD1(x)\n",
    "            x2 = self.convD2(x+x1)\n",
    "            x3 = self.convD3(x+x1+x2)\n",
    "       \n",
    "        return x + x1 + x2 + x3\n",
    "    \n",
    "##########################################################################\n",
    "# enhanced residual pixel-wise attention block (ERPAB)\n",
    "class ERPAB(nn.Module):\n",
    "    \"\"\" \n",
    "    Enhanced Residual Pixel-wise Attention Block \n",
    "    Usage:\n",
    "        self.erpab = ERPAB(in_channels=32, mid_channels=32, kernel=3, stride=1, d=[1, 2, 5], bias=False)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=32,\n",
    "                 mid_channels=32,\n",
    "                 kernel=3,\n",
    "                 stride=1,\n",
    "                 d=[1, 2, 5],\n",
    "                 bias=False):\n",
    "        super(ERPAB, self).__init__()\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[0], dilation=d[0], bias=bias),  # C32D1\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[1], dilation=d[1], bias=bias),  # C32D2\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel, stride, padding=d[2], dilation=d[2], bias=bias),  # C32D5\n",
    "        ])\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(mid_channels*3, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.attn_map = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, 1, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input feature map\n",
    "        Returns:\n",
    "            enhanced feature map\n",
    "        Usage:\n",
    "            enhanced_feature = ERPAB(input_feature)\n",
    "        \"\"\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            expert_outputs = torch.cat([expert(x) for expert in self.experts], dim=1)\n",
    "            x1 = F.relu(self.conv1(expert_outputs))\n",
    "            attn_map = self.attn_map(x1)\n",
    "\n",
    "        return x + x1 * self.sigmoid(attn_map)\n",
    "\n",
    "##########################################################################\n",
    "# cross-stage feature interaction module (CFIM)\n",
    "class CFIM(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Stage Feature Interaction Module\n",
    "    Usage:\n",
    "        self.cfim = CFIM(in_channels=32, norm_type = 'DyT' or 'WithBias' or 'BiasFree')\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, norm_type='DyT'):\n",
    "        super(CFIM, self).__init__()\n",
    "        self.norm1 = Norms.Norm(in_channels, norm_type)\n",
    "        self.norm2 = Norms.Norm(in_channels, norm_type)\n",
    "        self.rsconv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.rsconv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.drconv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.drconv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, r_net, dr_net):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rs_net: rain streaks removal network intermediate output\n",
    "            dr_net: details reconstruction network intermediate output\n",
    "        Returns:\n",
    "            to_rs_net: updated rain streaks removal network intermediate output\n",
    "            to_dr_net: updated details reconstruction network intermediate output\n",
    "        Usage:\n",
    "            to_rs_net, to_dr_net = CFIM(r_net, dr_net)\n",
    "        \"\"\"     \n",
    "        with torch.cuda.amp.autocast():\n",
    "            rs1 = self.rsconv1(self.norm1(r_net))\n",
    "            dr1 = self.drconv1(self.norm2(dr_net))\n",
    "            A = torch.matmul(rs1, dr1)\n",
    "            rs2 = self.rsconv2(rs1)\n",
    "            dr2 = self.drconv2(dr1)\n",
    "            rs_side = torch.matmul(A, rs2)\n",
    "            dr_side = torch.matmul(A, dr2)\n",
    "            to_rs_net = dr_side + r_net\n",
    "            to_dr_net = rs_side + dr_net\n",
    "\n",
    "        return to_rs_net, to_dr_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDRB Output Shape: torch.Size([1, 32, 64, 64])\n",
      "ERPAB Output Shape: torch.Size([1, 32, 64, 64])\n",
      "CFIM Output Shape to Rs Net: torch.Size([1, 32, 64, 64])\n",
      "CFIM Output Shape to Dr Net: torch.Size([1, 32, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\azurl\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "def test_modules():\n",
    "    x = torch.randn(1, 32, 64, 64)  # Example input\n",
    "    ddrb = DDRB(in_channels=32)\n",
    "    erpab = ERPAB(in_channels=32)\n",
    "    cfim = CFIM(in_channels=32)\n",
    "    \n",
    "    print(\"DDRB Output Shape:\", ddrb(x).shape)\n",
    "    print(\"ERPAB Output Shape:\", erpab(x).shape)\n",
    "    \n",
    "    x1 = torch.randn(1, 32, 64, 64)\n",
    "    x2 = torch.randn(1, 32, 64, 64)\n",
    "    to_rs_net, to_dr_net = cfim(x1, x2)\n",
    "    print(\"CFIM Output Shape to Rs Net:\", to_rs_net.shape)\n",
    "    print(\"CFIM Output Shape to Dr Net:\", to_dr_net.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################\n",
    "# DPENet_v2 with CFIM\n",
    "class DPENet_CFIM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 mid_channels=32,\n",
    "                 kernel=3,\n",
    "                 stride=1,\n",
    "                 dilation_list=[1, 2, 5],\n",
    "                 bias=False):\n",
    "        super(DPENet_CFIM, self).__init__()\n",
    "\n",
    "        # Initial feature transformation\n",
    "        self.inconv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.outconv1 = nn.Conv2d(mid_channels, in_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.inconv2 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.outconv2 = nn.Conv2d(mid_channels, in_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.inconv3 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.outconv3 = nn.Conv2d(mid_channels, in_channels, kernel_size=1, padding=0, bias=bias)\n",
    "\n",
    "        # Network Modules\n",
    "        self.ddrb1 = nn.Sequential(*[DDRB(mid_channels, mid_channels, kernel, stride, dilation_list, bias) for _ in range(5)])\n",
    "        self.ddrb2 = nn.Sequential(*[DDRB(mid_channels, mid_channels, kernel, stride, dilation_list, bias) for _ in range(5)])\n",
    "        \n",
    "        # Shared ERPAB instance\n",
    "        self.erpab1 = ERPAB(mid_channels, mid_channels, kernel, stride, dilation_list, bias)\n",
    "        self.erpab2 = nn.Sequential(*[ERPAB(mid_channels, mid_channels, kernel, stride, dilation_list, bias) for _ in range(2)])\n",
    "        \n",
    "        self.cfim = CFIM(mid_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ = x\n",
    "        \n",
    "        # Stage 1: Initial Rain Streaks Removal\n",
    "        x = self.inconv1(x)\n",
    "        rs1 = self.ddrb1(x)\n",
    "        x = self.outconv1(rs1)\n",
    "        x_mid = x + input_  # Residual connection\n",
    "        \n",
    "        # Stage 2: Initial Detail Reconstruction\n",
    "        x = self.inconv2(F.relu(x_mid))\n",
    "        dr1 = self.erpab1(x)\n",
    "        \n",
    "        # Cross-stage Feature Interaction\n",
    "        rs2, _ = self.cfim(rs1, dr1)\n",
    "        \n",
    "        # Stage 3: Further Rain Streaks Removal\n",
    "        x = self.ddrb2(rs2)\n",
    "        x = self.outconv2(x)\n",
    "        x_rain_removed = x + x_mid  # Residual connection\n",
    "        \n",
    "        # Stage 4: Further Detail Reconstruction\n",
    "        x = self.inconv3(x_rain_removed)\n",
    "        dr2 = self.erpab1(x)\n",
    "        \n",
    "        # Cross-stage Feature Interaction\n",
    "        _, dr3 = self.cfim(rs1, dr2)\n",
    "        \n",
    "        # Final Detail Enhancement\n",
    "        x = self.erpab2(dr3)\n",
    "        x = self.outconv3(x)\n",
    "        x_final = x + x_rain_removed  # Residual connection\n",
    "        \n",
    "        return x_rain_removed, x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (rain removed): torch.Size([1, 3, 64, 64])\n",
      "Output shape (final reconstruction): torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\azurl\\AppData\\Local\\Temp\\ipykernel_18036\\716924590.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "def test_dpenet_cfim():\n",
    "    model = DPENet_CFIM()\n",
    "    test_input = torch.randn(1, 3, 64, 64)  # Batch size = 1, 3 channels, 64x64 image\n",
    "    output_rain_removed, output_final = model(test_input)\n",
    "    print(\"Output shape (rain removed):\", output_rain_removed.shape)\n",
    "    print(\"Output shape (final reconstruction):\", output_final.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dpenet_cfim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
